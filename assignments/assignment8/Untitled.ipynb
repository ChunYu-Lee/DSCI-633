{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-versicolor', 'Iris-virginica', 'Iris-setosa']\n",
      "{'Iris-versicolor': {'TP': 44, 'TN': 85, 'FP': 5, 'FN': 1}, 'Iris-virginica': {'TP': 40, 'TN': 89, 'FP': 1, 'FN': 5}, 'Iris-setosa': {'TP': 45, 'TN': 84, 'FP': 6, 'FN': 0}}\n",
      "0.8823529411764706\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "\n",
    "data_train = pd.read_csv(\"../data/Iris_train.csv\")\n",
    "# Separate independent variables and dependent variables\n",
    "independent = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n",
    "X = data_train[independent]\n",
    "y = data_train[\"Species\"]\n",
    "# Fit model\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "clf.fit(X, y)\n",
    "# Predict on training data\n",
    "predictions = clf.predict(X)\n",
    "# Predict probabilities\n",
    "probs = clf.predict_proba(X)\n",
    "probs = pd.DataFrame({key: probs[:, i] for i, key in enumerate(clf.classes_)})\n",
    "pred_proba = None\n",
    "actuals = y\n",
    "\n",
    "if type(pred_proba)!= type(None):\n",
    "    classes_ = list(pred_proba.keys())\n",
    "else:\n",
    "    classes_ = list(set(list(predictions)+list(actuals)))\n",
    "confusion_matrix = None\n",
    "\n",
    "print(classes_)\n",
    "\n",
    "correct = predictions == actuals\n",
    "wrong = predictions != actuals\n",
    "\n",
    "\n",
    "acc = float(Counter(correct)[True])/len(correct)\n",
    "confusion_matrix = {}\n",
    "for label in classes_:\n",
    "    tp =  Counter(correct[actuals == label])[True]\n",
    "    fp =  Counter(wrong[actuals != label])[True]\n",
    "    tn =  Counter(correct[actuals != label])[True]\n",
    "    fn =  Counter(wrong[actuals == label])[True]\n",
    "    \n",
    "    confusion_matrix[label] = {\"TP\":tp, \"TN\": tn, \"FP\": fp, \"FN\": fn}\n",
    "print(confusion_matrix)\n",
    "\n",
    "if target in classes_:\n",
    "    tp = confusion_matrix[target][\"TP\"]\n",
    "    fp = confusion_matrix[target][\"FP\"]\n",
    "    if tp+fp == 0:\n",
    "        prec = 0\n",
    "    else:\n",
    "        prec = float(tp) / (tp + fp)\n",
    "else:\n",
    "    if average == \"micro\":\n",
    "        prec = acc\n",
    "    else:\n",
    "        prec = 0\n",
    "        n = len(actuals)\n",
    "        for label in classes_:\n",
    "            tp = confusion_matrix[label][\"TP\"]\n",
    "            fp = confusion_matrix[label][\"FP\"]\n",
    "            if tp + fp == 0:\n",
    "                prec_label = 0\n",
    "            else:\n",
    "                prec_label = float(tp) / (tp + fp)\n",
    "            if average == \"macro\":\n",
    "                ratio = 1 / len(classes_)\n",
    "            elif average == \"weighted\":\n",
    "                ratio = Counter(actuals)[label] / float(n)\n",
    "            else:\n",
    "                raise Exception(\"Unknown type of average.\")\n",
    "            prec += prec_label * ratio\n",
    "print(prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Iris-setosa': {'prec': 0.8823529411764706, 'recall': 1.0, 'f1': 0.9375, 'auc': 44.49999999999999}, 'Iris-versicolor': {'prec': 0.8979591836734694, 'recall': 0.9777777777777777, 'f1': 0.9361702127659575, 'auc': 44.48345679012345}, 'Iris-virginica': {'prec': 0.975609756097561, 'recall': 0.8888888888888888, 'f1': 0.9302325581395349, 'auc': 43.98814814814814}}\n",
      "Average F1 scores: \n",
      "{'macro': 0.9367345455622736, 'micro': 0.9555555555555556, 'weighted': 0.9367345455622736}\n"
     ]
    }
   ],
   "source": [
    "from my_evaluation_hint import my_evaluation\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "##################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load training data\n",
    "    data_train = pd.read_csv(\"../data/Iris_train.csv\")\n",
    "    # Separate independent variables and dependent variables\n",
    "    independent = [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n",
    "    X = data_train[independent]\n",
    "    y = data_train[\"Species\"]\n",
    "    # Fit model\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "    clf.fit(X, y)\n",
    "    # Predict on training data\n",
    "    predictions = clf.predict(X)\n",
    "    # Predict probabilities\n",
    "    probs = clf.predict_proba(X)\n",
    "    probs = pd.DataFrame({key: probs[:, i] for i, key in enumerate(clf.classes_)})\n",
    "    # Evaluate results\n",
    "    metrics = my_evaluation(predictions, y, probs)\n",
    "    result = {}\n",
    "    for target in clf.classes_:\n",
    "        result[target] = {}\n",
    "        result[target][\"prec\"] = metrics.precision(target)\n",
    "        result[target][\"recall\"] = metrics.recall(target)\n",
    "        result[target][\"f1\"] = metrics.f1(target)\n",
    "        result[target][\"auc\"] = metrics.auc(target)\n",
    "    print(result)\n",
    "    f1 = {average: metrics.f1(target=None, average=average) for average in [\"macro\", \"micro\", \"weighted\"]}\n",
    "    print(\"Average F1 scores: \")\n",
    "    print(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
